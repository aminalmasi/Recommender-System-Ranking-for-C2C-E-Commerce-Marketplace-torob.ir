{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommender System Ranking for C2C E-Commerce Marketplace.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Py_IlPst9wf"
      },
      "outputs": [],
      "source": [
        "!pip install parsivar\n",
        "!pip install git+https://github.com/RoboEpics/roboepics-client.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboepics_client.roboepics_client import RoboEpicsClient\n",
        "\n",
        "problem_id = 4\n",
        "problem_enter_id = 2003"
      ],
      "metadata": {
        "id": "S8KLxk7YuBHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1uYwzBe8nLhOQ2Q3rCScEXvljLkQqJrHc\n",
        "!7z x data.7z\n",
        "!ls"
      ],
      "metadata": {
        "id": "ebv4mNlZuAMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "\n",
        "import collections\n",
        "import gc\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import parsivar\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "QzJ7uBcLuIE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json(path, n_lines_to_read=None):\n",
        "\n",
        "    with open(path) as f:\n",
        "        for i, line in enumerate(tqdm(f)):\n",
        "            if n_lines_to_read == i:\n",
        "                break\n",
        "            yield json.loads(line)"
      ],
      "metadata": {
        "id": "3MIHFeKeuLif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsivar_normalizer = parsivar.Normalizer(statistical_space_correction=True)\n",
        "\n",
        "char_mappings = {\n",
        "    \"٥\": \"5\",\n",
        "    \"А\": \"a\",\n",
        "    \"В\": \"b\",\n",
        "    \"Е\": \"e\",\n",
        "    \"Н\": \"h\",\n",
        "    \"Р\": \"P\",\n",
        "    \"С\": \"C\",\n",
        "    \"Т\": \"T\",\n",
        "    \"а\": \"a\",\n",
        "    \"г\": \"r\",\n",
        "    \"е\": \"e\",\n",
        "    \"к\": \"k\",\n",
        "    \"м\": \"m\",\n",
        "    \"о\": \"o\",\n",
        "    \"р\": \"p\",\n",
        "    \"ڈ\": \"د\",\n",
        "    \"ڇ\": \"چ\",\n",
        "    \"۰\": \"0\",\n",
        "    \"۱\": \"1\",\n",
        "    \"۲\": \"2\",\n",
        "    \"۳\": \"3\",\n",
        "    \"۴\": \"4\",\n",
        "    \"۵\": \"5\",\n",
        "    \"۶\": \"6\",\n",
        "    \"۷\": \"7\",\n",
        "    \"۸\": \"8\",\n",
        "    \"۹\": \"9\",\n",
        "    \".\": \".\",\n",
        "    \"٠\": \"0\",\n",
        "    \"١\": \"1\",\n",
        "    \"٢\": \"2\",\n",
        "    \"٣\": \"3\",\n",
        "    \"٤\": \"4\",\n",
        "    \"٥\": \"5\",\n",
        "    \"٦\": \"6\",\n",
        "    \"٧\": \"7\",\n",
        "    \"٨\": \"8\",\n",
        "    \"٩\": \"9\",\n",
        "    \"ك\": \"ک\",\n",
        "    \"ى\": \"ی\",\n",
        "    \"ي\": \"ی\",\n",
        "    \"ؤ\": \"و\",\n",
        "    \"ئ\": \"ی\",\n",
        "    \"إ\": \"ا\",\n",
        "    \"أ\": \"ا\",\n",
        "    \"آ\": \"ا\",\n",
        "    \"ة\": \"ه\",\n",
        "    \"ء\": \"ی\",\n",
        "    \"à\": \"a\",\n",
        "    \"ä\": \"a\",\n",
        "    \"ç\": \"c\",\n",
        "    \"é\": \"e\",\n",
        "    \"è\": \"e\",\n",
        "    \"ê\": \"e\",\n",
        "    \"ë\": \"e\",\n",
        "    \"î\": \"i\",\n",
        "    \"ï\": \"i\",\n",
        "    \"ô\": \"o\",\n",
        "    \"ù\": \"u\",\n",
        "    \"û\": \"u\",\n",
        "    \"ü\": \"u\",\n",
        "    \",\": \".\",\n",
        "    \"&\": \" and \",\n",
        "    \"ّ\": \"\", \n",
        "    \"َ\": \"\", \n",
        "    \"ِ\": \"\", \n",
        "    \"ُ\": \"\", \n",
        "    \"ـ\": \"\",\n",
        "    \"‍\": \"\",\n",
        "    \"‌\": \" \",\n",
        "    \"ﭐ\": \"ا\",\n",
        "    \"ﭑ\": \"ا\",\n",
        "    \"ﭖ\": \"پ\",\n",
        "    \"ﭗ\": \"پ\",\n",
        "    \"ﭘ\": \"پ\",\n",
        "    \"ﭙ\": \"پ\",\n",
        "    \"ﭞ\": \"ت\",\n",
        "    \"ﭟ\": \"ت\",\n",
        "    \"ﭠ\": \"ت\",\n",
        "    \"ﭡ\": \"ت\",\n",
        "    \"ﭺ\": \"چ\",\n",
        "    \"ﭻ\": \"چ\",\n",
        "    \"ﭼ\": \"چ\",\n",
        "    \"ﭽ\": \"چ\",\n",
        "    \"ﮊ\": \"ژ\",\n",
        "    \"ﮋ\": \"ژ\",\n",
        "    \"ﮎ\": \"ک\",\n",
        "    \"ﮏ\": \"ک\",\n",
        "    \"ﮐ\": \"ک\",\n",
        "    \"ﮑ\": \"ک\",\n",
        "    \"ﮒ\": \"گ\",\n",
        "    \"ﮓ\": \"گ\",\n",
        "    \"ﮔ\": \"گ\",\n",
        "    \"ﮕ\": \"گ\",\n",
        "    \"ﮤ\": \"ه\",\n",
        "    \"ﮥ\": \"ه\",\n",
        "    \"ﮦ\": \"ه\",\n",
        "    \"ﮪ\": \"ه\",\n",
        "    \"ﮫ\": \"ه\",\n",
        "    \"ﮬ\": \"ه\",\n",
        "    \"ﮭ\": \"ه\",\n",
        "    \"ﮮ\": \"ی\",\n",
        "    \"ﮯ\": \"ی\",\n",
        "    \"ﮰ\": \"ی\",\n",
        "    \"ﮱ\": \"ی\",\n",
        "    \"ﯼ\": \"ی\",\n",
        "    \"ﯽ\": \"ی\",\n",
        "    \"ﯾ\": \"ی\",\n",
        "    \"ﯿ\": \"ی\",\n",
        "    \"ﹰ\": \"\",\n",
        "    \"ﹱ\": \"\",\n",
        "    \"ﹲ\": \"\",\n",
        "    \"ﹳ\": \"\",\n",
        "    \"ﹴ\": \"\",\n",
        "    \"﹵\": \"\",\n",
        "    \"ﹶ\": \"\",\n",
        "    \"ﹷ\": \"\",\n",
        "    \"ﹸ\": \"\",\n",
        "    \"ﹹ\": \"\",\n",
        "    \"ﹺ\": \"\",\n",
        "    \"ﹻ\": \"\",\n",
        "    \"ﹼ\": \"\",\n",
        "    \"ﹽ\": \"\",\n",
        "    \"ﹾ\": \"\",\n",
        "    \"ﹿ\": \"\",\n",
        "    \n",
        "    \"ﺀ\": \"ی\",\n",
        "    \"ﺁ\": \"ا\",\n",
        "    \"ﺂ\": \"ا\",\n",
        "    \"ﺃ\": \"ا\",\n",
        "    \"ﺄ\": \"ا\",\n",
        "    \"ﺅ\": \"و\",\n",
        "    \"ﺆ\": \"و\",\n",
        "    \"ﺇ\": \"ا\",\n",
        "    \"ﺈ\": \"ا\",\n",
        "    \"ﺉ\": \"ی\",\n",
        "    \"ﺊ\": \"ی\",\n",
        "    \"ﺋ\": \"ی\",\n",
        "    \"ﺌ\": \"ی\",\n",
        "    \"ﺍ\": \"ا\",\n",
        "    \"ﺎ\": \"ا\",\n",
        "    \"ﺏ\": \"ب\",\n",
        "    \"ﺐ\": \"ب\",\n",
        "    \"ﺑ\": \"ب\",\n",
        "    \"ﺒ\": \"ب\",\n",
        "    \"ﺓ\": \"ه\",\n",
        "    \"ﺔ\": \"ه\",\n",
        "    \"ﺕ\": \"ت\",\n",
        "    \"ﺖ\": \"ت\",\n",
        "    \"ﺗ\": \"ت\",\n",
        "    \"ﺘ\": \"ت\",\n",
        "    \"ﺙ\": \"ث\",\n",
        "    \"ﺚ\": \"ث\",\n",
        "    \"ﺛ\": \"ث\",\n",
        "    \"ﺜ\": \"ث\",\n",
        "    \"ﺝ\": \"ج\",\n",
        "    \"ﺞ\": \"ج\",\n",
        "    \"ﺟ\": \"ج\",\n",
        "    \"ﺠ\": \"ج\",\n",
        "    \"ﺡ\": \"ح\",\n",
        "    \"ﺢ\": \"ح\",\n",
        "    \"ﺣ\": \"ح\",\n",
        "    \"ﺤ\": \"ح\",\n",
        "    \"ﺥ\": \"خ\",\n",
        "    \"ﺦ\": \"خ\",\n",
        "    \"ﺧ\": \"خ\",\n",
        "    \"ﺨ\": \"خ\",\n",
        "    \"ﺩ\": \"د\",\n",
        "    \"ﺪ\": \"د\",\n",
        "    \"ﺫ\": \"ذ\",\n",
        "    \"ﺬ\": \"ذ\",\n",
        "    \"ﺭ\": \"ر\",\n",
        "    \"ﺮ\": \"ر\",\n",
        "    \"ﺯ\": \"ز\",\n",
        "    \"ﺰ\": \"ز\",\n",
        "    \"ﺱ\": \"س\",\n",
        "    \"ﺲ\": \"س\",\n",
        "    \"ﺳ\": \"س\",\n",
        "    \"ﺴ\": \"س\",\n",
        "    \"ﺵ\": \"ش\",\n",
        "    \"ﺶ\": \"ش\",\n",
        "    \"ﺷ\": \"ش\",\n",
        "    \"ﺸ\": \"ش\",\n",
        "    \"ﺹ\": \"ص\",\n",
        "    \"ﺺ\": \"ص\",\n",
        "    \"ﺻ\": \"ص\",\n",
        "    \"ﺼ\": \"ص\",\n",
        "    \"ﺽ\": \"ض\",\n",
        "    \"ﺾ\": \"ض\",\n",
        "    \"ﺿ\": \"ض\",\n",
        "    \"ﻀ\": \"ض\",\n",
        "    \"ﻁ\": \"ط\",\n",
        "    \"ﻂ\": \"ط\",\n",
        "    \"ﻃ\": \"ط\",\n",
        "    \"ﻄ\": \"ط\",\n",
        "    \"ﻅ\": \"ظ\",\n",
        "    \"ﻆ\": \"ظ\",\n",
        "    \"ﻇ\": \"ظ\",\n",
        "    \"ﻈ\": \"ظ\",\n",
        "    \"ﻉ\": \"ع\",\n",
        "    \"ﻊ\": \"ع\",\n",
        "    \"ﻋ\": \"ع\",\n",
        "    \"ﻌ\": \"ع\",\n",
        "    \"ﻍ\": \"غ\",\n",
        "    \"ﻎ\": \"غ\",\n",
        "    \"ﻏ\": \"غ\",\n",
        "    \"ﻐ\": \"غ\",\n",
        "    \"ﻑ\": \"ف\",\n",
        "    \"ﻒ\": \"ف\",\n",
        "    \"ﻓ\": \"ف\",\n",
        "    \"ﻔ\": \"ف\",\n",
        "    \"ﻕ\": \"ق\",\n",
        "    \"ﻖ\": \"ق\",\n",
        "    \"ﻗ\": \"ق\",\n",
        "    \"ﻘ\": \"ق\",\n",
        "    \"ﻙ\": \"ک\",\n",
        "    \"ﻚ\": \"ک\",\n",
        "    \"ﻛ\": \"ک\",\n",
        "    \"ﻜ\": \"ک\",\n",
        "    \"ﻝ\": \"ل\",\n",
        "    \"ﻞ\": \"ل\",\n",
        "    \"ﻟ\": \"ل\",\n",
        "    \"ﻠ\": \"ل\",\n",
        "    \"ﻡ\": \"م\",\n",
        "    \"ﻢ\": \"م\",\n",
        "    \"ﻣ\": \"م\",\n",
        "    \"ﻤ\": \"م\",\n",
        "    \"ﻥ\": \"ن\",\n",
        "    \"ﻦ\": \"ن\",\n",
        "    \"ﻧ\": \"ن\",\n",
        "    \"ﻨ\": \"ن\",\n",
        "    \"ﻩ\": \"ه\",\n",
        "    \"ﻪ\": \"ه\",\n",
        "    \"ﻫ\": \"ه\",\n",
        "    \"ﻬ\": \"ه\",\n",
        "    \"ﻭ\": \"و\",\n",
        "    \"ﻮ\": \"و\",\n",
        "    \"ﻯ\": \"ی\",\n",
        "    \"ﻰ\": \"ی\",\n",
        "    \"ﻱ\": \"ی\",\n",
        "    \"ﻲ\": \"ی\",\n",
        "    \"ﻳ\": \"ی\",\n",
        "    \"ﻴ\": \"ی\",\n",
        "    \"ﻵ\": \"لا\",\n",
        "    \"ﻶ\": \"لا\",\n",
        "    \"ﻷ\": \"لا\",\n",
        "    \"ﻸ\": \"لا\",\n",
        "    \"ﻹ\": \"لا\",\n",
        "    \"ﻺ\": \"لا\",\n",
        "    \"ﻻ\": \"لا\",\n",
        "    \"ﻼ\": \"لا\",\n",
        "}\n",
        "\n",
        "valid_chars = [\n",
        "    \" \",\n",
        "    \"0\",\n",
        "    \"1\",\n",
        "    \"2\",\n",
        "    \"3\",\n",
        "    \"4\",\n",
        "    \"5\",\n",
        "    \"6\",\n",
        "    \"7\",\n",
        "    \"8\",\n",
        "    \"9\",\n",
        "    \"A\",\n",
        "    \"B\",\n",
        "    \"C\",\n",
        "    \"D\",\n",
        "    \"E\",\n",
        "    \"F\",\n",
        "    \"G\",\n",
        "    \"H\",\n",
        "    \"I\",\n",
        "    \"J\",\n",
        "    \"K\",\n",
        "    \"L\",\n",
        "    \"M\",\n",
        "    \"N\",\n",
        "    \"O\",\n",
        "    \"P\",\n",
        "    \"Q\",\n",
        "    \"R\",\n",
        "    \"S\",\n",
        "    \"T\",\n",
        "    \"U\",\n",
        "    \"V\",\n",
        "    \"W\",\n",
        "    \"X\",\n",
        "    \"Y\",\n",
        "    \"Z\",\n",
        "    \"a\",\n",
        "    \"b\",\n",
        "    \"c\",\n",
        "    \"d\",\n",
        "    \"e\",\n",
        "    \"f\",\n",
        "    \"g\",\n",
        "    \"h\",\n",
        "    \"i\",\n",
        "    \"j\",\n",
        "    \"k\",\n",
        "    \"l\",\n",
        "    \"m\",\n",
        "    \"n\",\n",
        "    \"o\",\n",
        "    \"p\",\n",
        "    \"q\",\n",
        "    \"r\",\n",
        "    \"s\",\n",
        "    \"t\",\n",
        "    \"u\",\n",
        "    \"v\",\n",
        "    \"w\",\n",
        "    \"x\",\n",
        "    \"y\",\n",
        "    \"z\",\n",
        "    \"آ\",\n",
        "    \"ئ\",\n",
        "    \"ا\",\n",
        "    \"ب\",\n",
        "    \"ت\",\n",
        "    \"ث\",\n",
        "    \"ج\",\n",
        "    \"ح\",\n",
        "    \"خ\",\n",
        "    \"د\",\n",
        "    \"ذ\",\n",
        "    \"ر\",\n",
        "    \"ز\",\n",
        "    \"س\",\n",
        "    \"ش\",\n",
        "    \"ص\",\n",
        "    \"ض\",\n",
        "    \"ط\",\n",
        "    \"ظ\",\n",
        "    \"ع\",\n",
        "    \"غ\",\n",
        "    \"ف\",\n",
        "    \"ق\",\n",
        "    \"ل\",\n",
        "    \"م\",\n",
        "    \"ن\",\n",
        "    \"ه\",\n",
        "    \"و\",\n",
        "    \"پ\",\n",
        "    \"چ\",\n",
        "    \"ژ\",\n",
        "    \"ک\",\n",
        "    \"گ\",\n",
        "    \"ی\",\n",
        "]\n",
        "\n",
        "\n",
        "def _replace_rep(t):\n",
        "    \"Replace repetitions at the character level: ccc -> c\"\n",
        "\n",
        "    def __replace_rep(m):\n",
        "        c, cc = m.groups()\n",
        "        return f\"{c}\"\n",
        "\n",
        "    re_rep = re.compile(r\"(\\S)(\\1{2,})\")\n",
        "    return re_rep.sub(__replace_rep, t)\n",
        "\n",
        "\n",
        "def _replace_wrep(t):\n",
        "    \"Replace word repetitions: word word word -> word\"\n",
        "\n",
        "    def __replace_wrep(m):\n",
        "        c, cc = m.groups()\n",
        "        return f\"{c}\"\n",
        "\n",
        "    re_wrep = re.compile(r\"(\\b\\w+\\W+)(\\1{2,})\")\n",
        "    return re_wrep.sub(__replace_wrep, t)\n",
        "\n",
        "\n",
        "def _normalize_text(x):\n",
        "    \"\"\"normalize a sentence\"\"\"\n",
        "\n",
        "    x = str(x)\n",
        "    x = parsivar_normalizer.normalize(x)  # apply `parsivar` normalizations\n",
        "    x = re.sub(r\"[\\u200c\\r\\n]\", \" \", x)  # remove half space and new line characters\n",
        "    x = x.lower()\n",
        "    x = \"\".join(\n",
        "        [char_mappings[xx] if xx in char_mappings else xx for xx in x]\n",
        "    )  # substitue bad characters with appropriate ones\n",
        "    x = re.sub(\n",
        "        r\"[^{}]\".format(\"\".join(valid_chars)), \" \", x\n",
        "    )  # just keep valid characters and substitue others with space\n",
        "    x = re.sub(r\"[a-z]+\", r\" \\g<0> \", x)  # put space around words and numbers\n",
        "    x = re.sub(r\"[0-9]+\", r\" \\g<0> \", x)  # put space around words and numbers\n",
        "    x = re.sub(r\"\\s+\", \" \", x)  # remove more than one white spaces with space\n",
        "    x = _replace_rep(x)\n",
        "    x = _replace_wrep(x)\n",
        "    return x.strip()\n",
        "\n",
        "\n",
        "def normalize_texts(X, use_tqdm=False):\n",
        "    \"\"\"normalize list of sentences\"\"\"\n",
        "\n",
        "    if use_tqdm:\n",
        "        X = [_normalize_text(x) for x in tqdm(X)]\n",
        "    else:\n",
        "        X = [_normalize_text(x) for x in X]\n",
        "    return X"
      ],
      "metadata": {
        "id": "xTMlVzr3ubwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JSONListWriter:\n",
        "    \"\"\"\n",
        "    auxilary class to write list of dictionaries into json file.\n",
        "    each item in one line.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_path):\n",
        "        self.fd = None\n",
        "        self.file_path = file_path\n",
        "        self.delimiter = \"\\n\"\n",
        "\n",
        "    def open(self):\n",
        "        self.fd = open(self.file_path, \"w\")\n",
        "        self.first_item_written = False\n",
        "        return self\n",
        "\n",
        "    def close(self):\n",
        "        self.fd.close()\n",
        "        self.fd = None\n",
        "\n",
        "    def write_item(self, obj):\n",
        "        if self.first_item_written:\n",
        "            self.fd.write(self.delimiter)\n",
        "        self.fd.write(json.dumps(obj))\n",
        "        self.first_item_written = True\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self.open()\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        self.close()"
      ],
      "metadata": {
        "id": "hv0Q-AAHueZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_KkilhtT0b7"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIkcUDvoauZt"
      },
      "source": [
        "In this section we preprocess the data. \n",
        "It has the following steps:\n",
        "- set and extract a product name for each base product\n",
        "- normalize the product name\n",
        "- extract and exclude invalid products that don't have seller\n",
        "- aggregate clicks based on search_id\n",
        "- normalize raw_query\n",
        "- aggregate searches based on raw_query\n",
        "- aggregate results, clicks and page views on for each aggregated search\n",
        "- normalize offline test queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM23wPRU7aI7"
      },
      "source": [
        "data_folder = \"./data\"\n",
        "\n",
        "products_path = os.path.join(data_folder, \"base_products.json\")\n",
        "products_normalized_path = os.path.join(data_folder, \"base_products_normalized.json\")\n",
        "\n",
        "search_log_train_path = os.path.join(data_folder, \"search_log_train.json\")\n",
        "click_log_train_path = os.path.join(data_folder, \"click_log_train.json\")\n",
        "\n",
        "queries_test_offline_path = os.path.join(data_folder, \"queries_test_offline.json\")\n",
        "queries_test_offline_normalized_path = os.path.join(\n",
        "    data_folder, \"queries_test_offline_normalized.json\"\n",
        ")\n",
        "\n",
        "search_clicks_file_path = os.path.join(\n",
        "        data_folder, f\"searches_clicks_joined_train.json\"\n",
        "    )\n",
        "\n",
        "search_click_merged_path = os.path.join(data_folder, f\"searches_merged_train.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XajTzvVYpKxW"
      },
      "source": [
        "def make_base_product_names(products_path: str, products_normalized_path: str):\n",
        "\n",
        "    with JSONListWriter(products_normalized_path) as file:\n",
        "        for product in read_json(products_path):\n",
        "            pr_name = \"\"\n",
        "            for seller in product[\"sellers\"]:\n",
        "                pr_name += \" \" + seller[\"name1\"] + \" \" + seller[\"name2\"]\n",
        "            words = [w.strip() for w in pr_name.split()]\n",
        "            words = set(\n",
        "                [w for w in words if w != \"\"]\n",
        "            ) \n",
        "            pr_name = (\" \".join(words)).strip()\n",
        "\n",
        "            if (\n",
        "                pr_name == \"\"\n",
        "            ):\n",
        "                continue\n",
        "\n",
        "            product[\"product_name\"] = pr_name\n",
        "            product[\"product_name_normalized\"] = _normalize_text(pr_name)\n",
        "\n",
        "            file.write_item(product)\n",
        "\n",
        "\n",
        "def aggregate_clicks(search_path, click_path, tag, valid_base_ids):\n",
        "\n",
        "\n",
        "    search_clicks_dict = {}\n",
        "    for i, click_row in enumerate(\n",
        "        read_json(click_path)\n",
        "    ): \n",
        "        search_id = click_row[\"search_log_id\"]\n",
        "        base_product_id = click_row[\"base_product_id\"]\n",
        "\n",
        "        list_of_clicks = search_clicks_dict.get(search_id, [])\n",
        "        list_of_clicks.append(base_product_id)\n",
        "        search_clicks_dict[search_id] = list_of_clicks\n",
        "\n",
        "    invalid_results, invalid_clicks, invalid_searches = 0, 0, 0\n",
        "    \n",
        "    with JSONListWriter(\n",
        "        search_clicks_file_path\n",
        "    ) as file: \n",
        "        for i, search_row in enumerate(read_json(search_path)):\n",
        "            search_id = search_row[\"_id\"]\n",
        "            search_results = search_row[\"result\"]\n",
        "\n",
        "            results = [\n",
        "                r for r in search_results if r in valid_base_ids\n",
        "            ]  \n",
        "            results_set = set(results)\n",
        "\n",
        "            clicks = search_clicks_dict.get(search_id, [])\n",
        "            clicks = [\n",
        "                c for c in clicks if c in results_set\n",
        "            ]  \n",
        "\n",
        "            invalid_results += len(search_results) - len(results)\n",
        "            invalid_clicks += len(search_clicks_dict.get(search_id, [])) - len(clicks)\n",
        "\n",
        "            if len(clicks) == 0:\n",
        "                invalid_searches += 1\n",
        "                continue\n",
        "\n",
        "            search_row[\"raw_query\"] = search_row[\"raw_query\"].strip()\n",
        "            search_row[\"raw_query_normalized\"] = _normalize_text(\n",
        "                search_row[\"raw_query\"]\n",
        "            ) \n",
        "            search_row[\"result\"] = results\n",
        "            search_row[\"clicks\"] = clicks\n",
        "\n",
        "            file.write_item(search_row)\n",
        "\n",
        "    print(\n",
        "        f\"invalid searches: {invalid_searches}, \"\n",
        "        + f\"invalid results: {invalid_results}, \"\n",
        "        + f\"invalid clicks: {invalid_clicks}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def aggregate_searches(tag):\n",
        "    \"aggregates searches based on raw query.\"\n",
        "\n",
        "    search_clicks_path = os.path.join(data_folder, f\"searches_clicks_joined_{tag}.json\")\n",
        "    groups = {}\n",
        "    normalized_query_mapping = {}\n",
        "        raw_query = search[\"raw_query\"]\n",
        "        normalized_query_mapping[raw_query] = search[\"raw_query_normalized\"]\n",
        "\n",
        "        counters = groups.get(raw_query, {})\n",
        "        groups[raw_query] = counters\n",
        "\n",
        "        counters.setdefault(\"results\", collections.Counter())\n",
        "        counters.setdefault(\"pages\", collections.Counter())\n",
        "        counters.setdefault(\"clicks\", collections.Counter())\n",
        "\n",
        "        counters[\"results\"].update(search[\"result\"])\n",
        "        counters[\"pages\"].update([search[\"page\"]])\n",
        "        counters[\"clicks\"].update(search[\"clicks\"])\n",
        "\n",
        "    new_df = []\n",
        "    for raw_query, counters in tqdm(groups.items()):\n",
        "        results_counter = counters[\"results\"].most_common()  \n",
        "        pages_counter = counters[\"pages\"].most_common()  \n",
        "        clicks_counter = counters[\"clicks\"].most_common()  \n",
        "\n",
        "        new_df.append(\n",
        "            {\n",
        "                \"raw_query\": raw_query,\n",
        "                \"raw_query_normalized\": normalized_query_mapping[raw_query],\n",
        "                \"results\": [k for k, v in results_counter],\n",
        "                \"result_counts\": [v for k, v in results_counter],\n",
        "                \"pages\": [k for k, v in pages_counter],\n",
        "                \"page_counts\": [v for k, v in pages_counter],\n",
        "                \"clicks\": [k for k, v in clicks_counter],\n",
        "                \"click_counts\": [v for k, v in clicks_counter],\n",
        "            }\n",
        "        )\n",
        "    print(\"Number of unique queries after merge:\", len(new_df))\n",
        "\n",
        "    pd.DataFrame(new_df).to_json(\n",
        "        search_click_merged_path,\n",
        "        orient=\"records\",\n",
        "        lines=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def normalize_test_queries(queries_test_path, queries_test_normalized_path):\n",
        "      with JSONListWriter(queries_test_normalized_path) as file:\n",
        "        for query in read_json(queries_test_path):\n",
        "            normalized_query = _normalize_text(query)\n",
        "            file.write_item(normalized_query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_base_product_names(products_path, products_normalized_path)\n",
        "\n",
        "print(\"\\nProduct names created and saved in:\", products_normalized_path)"
      ],
      "metadata": {
        "id": "ZUnjVe7jvVYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_base_ids = set(\n",
        "    [product[\"_id\"] for product in read_json(products_normalized_path)]\n",
        ")\n",
        "print(\"\\nList of valid products created\")"
      ],
      "metadata": {
        "id": "6o9wQV1TvW_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggregate_clicks(\n",
        "    search_path=search_log_train_path,\n",
        "    click_path=click_log_train_path,\n",
        "    tag=\"train\",\n",
        "    valid_base_ids=valid_base_ids,\n",
        ")\n",
        "print(\"\\nSearches and clicks in the training set are merged\")"
      ],
      "metadata": {
        "id": "r_3u6DpJvaXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggregate_searches(\"train\")\n",
        "print(\"\\nSearches are aggregated wrt the raw query\")\n"
      ],
      "metadata": {
        "id": "rUDisyrevcNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RHuf54XUMSE"
      },
      "source": [
        "## Extracting features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "038vh1rbauZ9"
      },
      "source": [
        "In this section we extract feature for each product and searched query.\n",
        "At the end, a `dat` file is created which is the training data of LambdaMart model.\n",
        "\n",
        "Here, as a baseline we embed a sentences as follows:\n",
        "- extract the tf-idf of the setence\n",
        "- project the tf-idf vector into a lower dimension space by a random projection\n",
        "\n",
        "So, here is overview of the steps in this section:\n",
        "- embed product names in a low dimension vector\n",
        "- embed train raw_queries in a low dimension vector\n",
        "- embed offline test raw_queries in a low dimension vector\n",
        "- create `dat` file for model training\n",
        "- store embeded product names and queries in a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SVR5dfKsZME"
      },
      "source": [
        "import gc\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "\n",
        "def read_json(path, n_lines_to_read=None):\n",
        "\n",
        "    with open(path) as f:\n",
        "        for i, line in enumerate(tqdm(f)):\n",
        "            if n_lines_to_read == i:\n",
        "                break\n",
        "            yield json.loads(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5wz-rQZ0qUS"
      },
      "source": [
        "data_folder = \"./data\"\n",
        "\n",
        "merged_searches_path_train = os.path.join(data_folder, \"searches_merged_train.json\")\n",
        "\n",
        "products_path = os.path.join(data_folder, \"base_products.json\")\n",
        "products_normalized_path = os.path.join(data_folder, \"base_products_normalized.json\")\n",
        "\n",
        "queries_path_test = os.path.join(data_folder, \"queries_test_offline.json\")\n",
        "queries_normalized_path_test = os.path.join(\n",
        "    data_folder, \"queries_test_offline_normalized.json\"\n",
        ")\n",
        "\n",
        "random_projection_path = os.path.join(data_folder, \"random_projection.npz\")\n",
        "product_features_path = os.path.join(data_folder, \"product_features.npz\")\n",
        "query_train_features_path = os.path.join(data_folder, \"query_train_features.npz\")\n",
        "query_test_features_path = os.path.join(data_folder, \"query_test_features.npz\")\n",
        "\n",
        "train_dat_path = os.path.join(data_folder, f\"train_emb.dat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHM1QRW3XNYY"
      },
      "source": [
        "vocab_size = 4000 \n",
        "emb_dim = 256\n",
        "\n",
        "sample_num_train = 10000\n",
        "sample_num_test = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGW04fI5XPu-"
      },
      "source": [
        "print(\"read base products!\")\n",
        "products_df = pd.read_json(products_normalized_path, orient=\"records\", lines=True)\n",
        "products_df = products_df.drop(\"sellers\", axis=1)\n",
        "product_id_dict = {_id: ind for ind, _id in enumerate(products_df[\"_id\"])}\n",
        "product_names = products_df[\"product_name\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ZM6yYl06eV"
      },
      "source": [
        "print(\"read merged searches!\")\n",
        "merged_searches_train_df = list(read_json(merged_searches_path_train, sample_num_train))\n",
        "merged_searches_train_df = pd.DataFrame(merged_searches_train_df)\n",
        "queries_train = merged_searches_train_df[\"raw_query\"]\n",
        "queries_test = list(read_json(queries_path_test, sample_num_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74OBhsUz1F97"
      },
      "source": [
        "product_names_normalized = products_df[\"product_name_normalized\"]\n",
        "queries_train_normalized = merged_searches_train_df[\"raw_query_normalized\"]\n",
        "queries_test_normalized = list(read_json(queries_normalized_path_test, sample_num_test))\n",
        "\n",
        "del products_df  \n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzy5qf7i1Gfl"
      },
      "source": [
        "random_projection_mat = np.random.rand(vocab_size, emb_dim)  # random projection matrix\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=vocab_size, lowercase=True, use_idf=True)  # tfidf vectorizer\n",
        "vectorizer.fit(product_names_normalized)  # fit vectorizer\n",
        "\n",
        "# transform product names with tfidf vectorizer\n",
        "products_tfidf = vectorizer.transform(product_names_normalized)  \n",
        "# project the tfidf vector with random projection matrix\n",
        "products_projected = products_tfidf.dot(random_projection_mat)\n",
        "del products_tfidf  # free memory\n",
        "gc.collect()\n",
        "\n",
        "queries_train_tfidf = vectorizer.transform(queries_train_normalized)\n",
        "queries_train_projected = queries_train_tfidf.dot(random_projection_mat)\n",
        "del queries_train_tfidf\n",
        "gc.collect()\n",
        "\n",
        "queries_test_tfidf = vectorizer.transform(queries_test_normalized)\n",
        "queries_test_projected = queries_test_tfidf.dot(random_projection_mat)\n",
        "del queries_test_tfidf\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lU2f_gPUwQ9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqtHHFax1HyH"
      },
      "source": [
        "def make_dat_file(\n",
        "    dat_file_path,\n",
        "    merged_searches,\n",
        "    query_features,\n",
        "    product_features,\n",
        "    n_candidates=None,\n",
        "):\n",
        "  \n",
        "    \"\"\"\n",
        "    more information:\n",
        "     - https://xgboost.readthedocs.io/en/latest/tutorials/input_format.html#embedding-additional-information-inside-libsvm-file\n",
        "     - https://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html\n",
        "    \"\"\"\n",
        "    \n",
        "    features_list = []\n",
        "    scores = []\n",
        "    groups = []\n",
        "\n",
        "    with open(dat_file_path, \"w\") as file:\n",
        "        for qid, (_, merged_search) in enumerate(tqdm(merged_searches.iterrows())):\n",
        "            if n_candidates is None:\n",
        "                limit = len(merged_search[\"results\"])\n",
        "            limit = min(limit, len(merged_search[\"results\"]))\n",
        "            clicks = dict(zip(merged_search[\"clicks\"], merged_search[\"click_counts\"]))\n",
        "\n",
        "            for candidate_product_id in merged_search[\"results\"][:limit]:\n",
        "                candidate_score = clicks.get(candidate_product_id, 0)\n",
        "                candidate_score = np.log2(candidate_score + 1)\n",
        "\n",
        "                p_idx = product_id_dict[candidate_product_id]\n",
        "                feature = np.concatenate((product_features[p_idx], query_features[qid]))\n",
        "                feature = np.around(feature, 3)\n",
        "\n",
        "                file.write(\n",
        "                    f\"{candidate_score} qid:{qid} \"\n",
        "                    + \" \".join([f\"{i}:{s}\" for i, s in enumerate(feature)])\n",
        "                    + \"\\n\"\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeK0qqnR1JUB"
      },
      "source": [
        "make_dat_file(\n",
        "    train_dat_path,\n",
        "    merged_searches_train_df,\n",
        "    queries_train_projected,\n",
        "    products_projected,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCIAEw_i1K6I"
      },
      "source": [
        "np.savez(random_projection_path, random_projection_mat)\n",
        "np.savez(product_features_path, products_projected)\n",
        "np.savez(query_train_features_path, queries_train_projected)\n",
        "np.savez(query_test_features_path, queries_test_projected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "qs2fgIRkwv95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd_5diAQ1RBa"
      },
      "source": [
        "data_folder = \"./data\"\n",
        "\n",
        "train_dat_path = os.path.join(data_folder, f\"train_emb.dat\")\n",
        "model_path = os.path.join(data_folder, \"ranker.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twii74Fb1SCn"
      },
      "source": [
        "train_data = xgb.DMatrix(train_dat_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw9f7VWX1V-u"
      },
      "source": [
        "param = {\n",
        "    \"max_depth\": 20,\n",
        "    \"eta\": 0.3,\n",
        "    \"objective\": \"rank:ndcg\",\n",
        "    \"verbosity\": 1,\n",
        "    \"num_parallel_tree\": 1,\n",
        "    \"tree_method\": \"gpu_hist\",\n",
        "    \"eval_metric\": [\"ndcg\", \"ndcg@10\"],\n",
        "}\n",
        "\n",
        "eval_list = [(train_data, \"train\")]\n",
        "\n",
        "model = xgb.train(\n",
        "    param,\n",
        "    train_data,\n",
        "    num_boost_round=200,\n",
        "    evals=eval_list,\n",
        ")\n",
        "\n",
        "model.save_model(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}